# -*- coding: utf-8 -*-
"""Statistical Analysis Hindi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EgO4-ISE_wMTQqbnJtV2IPJggJo7JByj
"""

"""
Statistical Significance Analysis for Hindi Sentiment Analysis Models
=====================================================================
This script performs comprehensive statistical testing including:
1. Bootstrap confidence intervals (10,000 iterations)
2. McNemar's test for paired nominal data
3. Paired t-test
4. Effect size calculations (Cohen's d)
5. Comprehensive verification and visualization
6. Generate tables

"""
from google.colab import drive
import pandas as pd
import numpy as np

# Mount Google Drive
drive.mount('/content/gdrive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.stats.contingency_tables import mcnemar
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix,
    f1_score
)
from sklearn.utils import resample
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# ============================================================================
# SECTION 1: LOAD AND VERIFY DATA
# ============================================================================

def load_predictions(file_path, model_name):
    """
    Load predictions from CSV file and verify format
    """
    print(f"\n{'='*70}")
    print(f"Loading {model_name} predictions from:")
    print(f"  {file_path}")
    print(f"{'='*70}")

    try:
        df = pd.read_csv(file_path)

        # Display DataFrame info
        print(f"\n‚úÖ File loaded successfully")
        print(f"   Shape: {df.shape}")
        print(f"   Columns: {df.columns.tolist()}")
        print(f"\n   First 5 rows:")
        print(df.head())

        return df

    except FileNotFoundError:
        print(f"\n‚ùå ERROR: File not found at {file_path}")
        return None
    except Exception as e:
        print(f"\n‚ùå ERROR loading file: {e}")
        return None


def extract_predictions_and_labels(df, model_name):
    """
    Extract predictions and true labels from DataFrame
    Handles the specific column naming: True_Label and Predicted_Label
    Converts string labels to numeric (0, 1)
    """
    print(f"\nüîç Extracting predictions and labels for {model_name}...")

    # Based on your file structure
    label_col = 'True_Label'
    pred_col = 'Predicted_Label'

    if label_col not in df.columns or pred_col not in df.columns:
        print(f"\n‚ö†Ô∏è  Expected columns not found!")
        print(f"   Available columns: {df.columns.tolist()}")
        return None, None, None, None

    predictions = df[pred_col].values
    labels = df[label_col].values

    print(f"   ‚úÖ Predictions column: '{pred_col}'")
    print(f"   ‚úÖ Labels column: '{label_col}'")
    print(f"   ‚úÖ Length: {len(predictions)}")
    print(f"   ‚úÖ Unique predictions (raw): {np.unique(predictions)}")
    print(f"   ‚úÖ Unique labels (raw): {np.unique(labels)}")

    # Convert string labels to numeric if needed
    label_mapping = {
        'Negative': 0, 'negative': 0, 'NEGATIVE': 0, 'neg': 0, 'NEG': 0,
        'Positive': 1, 'positive': 1, 'POSITIVE': 1, 'pos': 1, 'POS': 1,
        0: 0, 1: 1, '0': 0, '1': 1
    }

    # Check if conversion is needed
    if isinstance(predictions[0], str) or isinstance(labels[0], str):
        print(f"   üîÑ Converting string labels to numeric...")

        try:
            predictions = np.array([label_mapping[p] for p in predictions])
            labels = np.array([label_mapping[l] for l in labels])
            print(f"   ‚úÖ Conversion successful!")
            print(f"   ‚úÖ Unique predictions (numeric): {np.unique(predictions)}")
            print(f"   ‚úÖ Unique labels (numeric): {np.unique(labels)}")
        except KeyError as e:
            print(f"   ‚ùå ERROR: Unknown label value found: {e}")
            print(f"   Please check your data for unexpected label values")
            return None, None, None, None

    # Also show the Correct_Prediction column stats if available
    if 'Correct_Prediction' in df.columns:
        correct_count = df['Correct_Prediction'].sum()
        accuracy = correct_count / len(df)
        print(f"   ‚úÖ Correct predictions: {correct_count}/{len(df)} ({accuracy*100:.2f}%)")

    # Verify the conversion worked
    calculated_accuracy = accuracy_score(labels, predictions)
    print(f"   ‚úÖ Calculated accuracy: {calculated_accuracy:.4f} ({calculated_accuracy*100:.2f}%)")

    return predictions, labels, pred_col, label_col


# ============================================================================
# SECTION 2: DATA VERIFICATION
# ============================================================================

def verify_data_alignment(predictions_dict, labels):
    """
    Verify all predictions are aligned with same test set
    """
    print(f"\n{'='*70}")
    print("DATA ALIGNMENT VERIFICATION")
    print(f"{'='*70}")

    all_valid = True
    n_samples = len(labels)

    print(f"\n‚úÖ True labels loaded: {n_samples} samples")
    print(f"   Negative (0): {(labels==0).sum()}")
    print(f"   Positive (1): {(labels==1).sum()}")
    print(f"   Class balance: {(labels==1).sum() / n_samples * 100:.2f}% positive")

    # Verify each model's predictions
    for model_name, preds in predictions_dict.items():
        print(f"\nüìÅ Checking {model_name}...")

        # Check 1: Length match
        if len(preds) != n_samples:
            print(f"   ‚ùå ERROR: Length mismatch!")
            print(f"      Expected: {n_samples}, Got: {len(preds)}")
            all_valid = False
            continue
        else:
            print(f"   ‚úÖ Length correct: {len(preds)} samples")

        # Check 2: Valid labels
        unique_preds = np.unique(preds)
        if not np.all(np.isin(unique_preds, [0, 1])):
            print(f"   ‚ùå ERROR: Invalid labels found: {unique_preds}")
            all_valid = False
            continue
        else:
            print(f"   ‚úÖ Valid labels: {unique_preds}")

        # Check 3: Calculate accuracy
        accuracy = accuracy_score(labels, preds)
        print(f"   ‚úÖ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

        # Check 4: Prediction distribution
        n_pos = (preds == 1).sum()
        n_neg = (preds == 0).sum()
        print(f"   üìä Predicted negative: {n_neg} ({n_neg/n_samples*100:.1f}%)")
        print(f"   üìä Predicted positive: {n_pos} ({n_pos/n_samples*100:.1f}%)")

    # Cross-check between models
    if len(predictions_dict) >= 2:
        print(f"\n{'='*70}")
        print("CROSS-MODEL AGREEMENT ANALYSIS")
        print(f"{'='*70}")

        model_names = list(predictions_dict.keys())
        for i in range(len(model_names)):
            for j in range(i+1, len(model_names)):
                name1, name2 = model_names[i], model_names[j]
                preds1, preds2 = predictions_dict[name1], predictions_dict[name2]

                # Calculate agreement
                agreement = (preds1 == preds2).sum()
                agreement_pct = agreement / n_samples * 100

                # Calculate confusion between models
                both_correct = ((preds1 == labels) & (preds2 == labels)).sum()
                model1_only = ((preds1 == labels) & (preds2 != labels)).sum()
                model2_only = ((preds1 != labels) & (preds2 == labels)).sum()
                both_wrong = ((preds1 != labels) & (preds2 != labels)).sum()

                print(f"\n{name1} vs {name2}:")
                print(f"   Agreement: {agreement}/{n_samples} ({agreement_pct:.2f}%)")
                print(f"   Both correct: {both_correct}")
                print(f"   Only {name1} correct: {model1_only}")
                print(f"   Only {name2} correct: {model2_only}")
                print(f"   Both wrong: {both_wrong}")

                # Sanity check
                total = both_correct + model1_only + model2_only + both_wrong
                if total != n_samples:
                    print(f"   ‚ùå ERROR: Counts don't add up! ({total} != {n_samples})")
                    all_valid = False

    print(f"\n{'='*70}")
    if all_valid:
        print("‚úÖ ALL CHECKS PASSED - Data is properly aligned!")
    else:
        print("‚ùå ERRORS FOUND - Fix issues before proceeding!")
    print(f"{'='*70}")

    return all_valid


# ============================================================================
# SECTION 3: STATISTICAL TESTS
# ============================================================================

def bootstrap_confidence_interval(predictions, true_labels,
                                   metric_func=accuracy_score,
                                   n_iterations=10000,
                                   confidence_level=0.95):
    """
    Calculate bootstrap confidence interval for a performance metric
    """
    bootstrapped_scores = []
    n_samples = len(predictions)

    print(f"   Running {n_iterations} bootstrap iterations...", end='', flush=True)

    for i in range(n_iterations):
        # Sample with replacement
        indices = resample(range(n_samples), n_samples=n_samples, random_state=i)

        # Calculate metric on bootstrap sample
        score = metric_func(true_labels[indices], predictions[indices])
        bootstrapped_scores.append(score)

        # Progress indicator
        if (i+1) % 2000 == 0:
            print(f".{i+1}", end='', flush=True)

    print(" Done!")

    # Calculate percentile confidence interval
    alpha = (1 - confidence_level) / 2
    lower_percentile = alpha * 100
    upper_percentile = (1 - alpha) * 100

    ci_lower = np.percentile(bootstrapped_scores, lower_percentile)
    ci_upper = np.percentile(bootstrapped_scores, upper_percentile)
    mean_score = np.mean(bootstrapped_scores)

    return {
        'mean': mean_score,
        'ci_lower': ci_lower,
        'ci_upper': ci_upper,
        'std': np.std(bootstrapped_scores),
        'bootstrap_distribution': bootstrapped_scores
    }


def mcnemars_test(model1_predictions, model2_predictions, true_labels):
    """
    Perform McNemar's test for paired nominal data
    Tests null hypothesis: both models have same error rate
    """
    # Determine correctness for each model
    model1_correct = (model1_predictions == true_labels)
    model2_correct = (model2_predictions == true_labels)

    # Create contingency table
    both_correct = np.sum(model1_correct & model2_correct)
    model1_only = np.sum(model1_correct & ~model2_correct)
    model2_only = np.sum(~model1_correct & model2_correct)
    both_wrong = np.sum(~model1_correct & ~model2_correct)

    # Contingency table for McNemar's test
    # Format: [[both_correct, model1_only], [model2_only, both_wrong]]
    contingency_table = np.array([[both_correct, model1_only],
                                   [model2_only, both_wrong]])

    # Perform McNemar's test with continuity correction
    result = mcnemar(contingency_table, exact=False, correction=True)

    # Calculate effect size (odds ratio)
    if model1_only + model2_only > 0:
        odds_ratio = model1_only / model2_only if model2_only > 0 else float('inf')
    else:
        odds_ratio = 1.0

    return {
        'statistic': result.statistic,
        'p_value': result.pvalue,
        'contingency_table': contingency_table,
        'both_correct': both_correct,
        'model1_only_correct': model1_only,
        'model2_only_correct': model2_only,
        'both_wrong': both_wrong,
        'odds_ratio': odds_ratio,
        'significant': result.pvalue < 0.05
    }


def paired_t_test(model1_predictions, model2_predictions, true_labels):
    """
    Perform paired t-test between two models
    """
    # Calculate per-sample accuracy (1 if correct, 0 if wrong)
    model1_correct = (model1_predictions == true_labels).astype(int)
    model2_correct = (model2_predictions == true_labels).astype(int)

    # Calculate differences
    differences = model1_correct - model2_correct

    # Perform paired t-test
    t_statistic, p_value = stats.ttest_rel(model1_correct, model2_correct)

    # Calculate confidence interval for mean difference
    mean_diff = np.mean(differences)
    se_diff = stats.sem(differences)
    ci = stats.t.interval(0.95, len(differences)-1,
                          loc=mean_diff, scale=se_diff)

    # Calculate Cohen's d (effect size)
    std_diff = np.std(differences, ddof=1)
    cohens_d = mean_diff / std_diff if std_diff > 0 else 0

    return {
        't_statistic': t_statistic,
        'p_value': p_value,
        'mean_difference': mean_diff,
        'confidence_interval_95': ci,
        'cohens_d': cohens_d,
        'significant': p_value < 0.05
    }


def bootstrap_comparison(model1_preds, model2_preds, true_labels,
                         n_iterations=10000):
    """
    Compare two models using bootstrap resampling
    Returns probability that model1 is better than model2
    """
    model1_better_count = 0
    differences = []
    n_samples = len(true_labels)

    print(f"   Running {n_iterations} bootstrap comparisons...", end='', flush=True)

    for i in range(n_iterations):
        indices = resample(range(n_samples), n_samples=n_samples, random_state=i)

        score1 = accuracy_score(true_labels[indices], model1_preds[indices])
        score2 = accuracy_score(true_labels[indices], model2_preds[indices])

        diff = score1 - score2
        differences.append(diff)

        if score1 > score2:
            model1_better_count += 1

        if (i+1) % 2000 == 0:
            print(f".{i+1}", end='', flush=True)

    print(" Done!")

    prob_model1_better = model1_better_count / n_iterations

    # Calculate CI for the difference
    diff_ci_lower = np.percentile(differences, 2.5)
    diff_ci_upper = np.percentile(differences, 97.5)
    mean_diff = np.mean(differences)

    return {
        'prob_model1_better': prob_model1_better,
        'prob_model2_better': 1 - prob_model1_better,
        'mean_difference': mean_diff,
        'diff_ci_lower': diff_ci_lower,
        'diff_ci_upper': diff_ci_upper,
        'significant_at_0.05': prob_model1_better > 0.975 or prob_model1_better < 0.025,
        'differences_distribution': differences
    }


# ============================================================================
# SECTION 4: COMPREHENSIVE ANALYSIS
# ============================================================================

def comprehensive_statistical_analysis(models_dict, true_labels,
                                       n_bootstrap=10000):
    """
    Perform comprehensive statistical comparison of multiple models
    """
    results = {
        'bootstrap_ci': {},
        'pairwise_mcnemar': {},
        'pairwise_t_test': {},
        'pairwise_bootstrap_comparison': {},
        'basic_metrics': {}
    }

    model_names = list(models_dict.keys())

    # ========================================
    # 0. Calculate Basic Metrics
    # ========================================
    print(f"\n{'='*70}")
    print("BASIC PERFORMANCE METRICS")
    print(f"{'='*70}")

    for name, preds in models_dict.items():
        print(f"\n{name}:")

        acc = accuracy_score(true_labels, preds)
        precision, recall, f1, support = precision_recall_fscore_support(
            true_labels, preds, average='weighted', zero_division=0
        )

        results['basic_metrics'][name] = {
            'accuracy': acc,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }

        print(f"   Accuracy:  {acc:.4f} ({acc*100:.2f}%)")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall:    {recall:.4f}")
        print(f"   F1-Score:  {f1:.4f}")

    # ========================================
    # 1. Bootstrap Confidence Intervals
    # ========================================
    print(f"\n{'='*70}")
    print(f"BOOTSTRAP CONFIDENCE INTERVALS (95%, {n_bootstrap} iterations)")
    print(f"{'='*70}")

    for name, preds in models_dict.items():
        print(f"\n{name}:")
        ci = bootstrap_confidence_interval(preds, true_labels,
                                           n_iterations=n_bootstrap)
        results['bootstrap_ci'][name] = ci

        print(f"   Accuracy: {ci['mean']:.4f} [{ci['ci_lower']:.4f}, {ci['ci_upper']:.4f}]")
        print(f"   Std Dev:  {ci['std']:.4f}")
        print(f"   CI Width: {ci['ci_upper'] - ci['ci_lower']:.4f}")

    # ========================================
    # 2. Pairwise McNemar's Tests
    # ========================================
    print(f"\n{'='*70}")
    print("PAIRWISE McNEMAR'S TESTS")
    print(f"{'='*70}")
    print("\nMcNemar's test evaluates whether two models have significantly")
    print("different error rates using a chi-squared test on the contingency table.")
    print("H0: Both models have the same error rate")

    for i in range(len(model_names)):
        for j in range(i+1, len(model_names)):
            name1, name2 = model_names[i], model_names[j]
            preds1, preds2 = models_dict[name1], models_dict[name2]

            mcnemar_result = mcnemars_test(preds1, preds2, true_labels)
            results['pairwise_mcnemar'][f"{name1}_vs_{name2}"] = mcnemar_result

            print(f"\n{name1} vs {name2}:")
            print(f"   œá¬≤ statistic:        {mcnemar_result['statistic']:.4f}")
            print(f"   p-value:             {mcnemar_result['p_value']:.4f}")
            print(f"   Significant (Œ±=0.05): {'YES ‚úì' if mcnemar_result['significant'] else 'NO ‚úó'}")
            print(f"   Only {name1} correct: {mcnemar_result['model1_only_correct']}")
            print(f"   Only {name2} correct: {mcnemar_result['model2_only_correct']}")
            print(f"   Both correct:        {mcnemar_result['both_correct']}")
            print(f"   Both wrong:          {mcnemar_result['both_wrong']}")

            if mcnemar_result['odds_ratio'] != float('inf'):
                print(f"   Odds ratio:          {mcnemar_result['odds_ratio']:.4f}")

    # ========================================
    # 3. Pairwise t-tests
    # ========================================
    print(f"\n{'='*70}")
    print("PAIRWISE PAIRED T-TESTS")
    print(f"{'='*70}")
    print("\nPaired t-test compares the mean difference in per-sample correctness.")
    print("H0: Mean difference between models equals zero")

    for i in range(len(model_names)):
        for j in range(i+1, len(model_names)):
            name1, name2 = model_names[i], model_names[j]
            preds1, preds2 = models_dict[name1], models_dict[name2]

            t_result = paired_t_test(preds1, preds2, true_labels)
            results['pairwise_t_test'][f"{name1}_vs_{name2}"] = t_result

            print(f"\n{name1} vs {name2}:")
            print(f"   t-statistic:          {t_result['t_statistic']:.4f}")
            print(f"   p-value:              {t_result['p_value']:.4f}")
            print(f"   Mean difference:      {t_result['mean_difference']:.4f}")
            print(f"   95% CI:               [{t_result['confidence_interval_95'][0]:.4f}, {t_result['confidence_interval_95'][1]:.4f}]")
            print(f"   Cohen's d:            {t_result['cohens_d']:.4f}")
            print(f"   Significant (Œ±=0.05): {'YES ‚úì' if t_result['significant'] else 'NO ‚úó'}")

            # Interpret effect size
            abs_d = abs(t_result['cohens_d'])
            if abs_d < 0.2:
                effect = "negligible"
            elif abs_d < 0.5:
                effect = "small"
            elif abs_d < 0.8:
                effect = "medium"
            else:
                effect = "large"
            print(f"   Effect size:          {effect}")

    # ========================================
    # 4. Bootstrap Comparison Probabilities
    # ========================================
    print(f"\n{'='*70}")
    print(f"BOOTSTRAP PAIRWISE COMPARISON ({n_bootstrap} iterations)")
    print(f"{'='*70}")
    print("\nBootstrap comparison estimates probability one model outperforms another.")

    for i in range(len(model_names)):
        for j in range(i+1, len(model_names)):
            name1, name2 = model_names[i], model_names[j]
            preds1, preds2 = models_dict[name1], models_dict[name2]

            print(f"\n{name1} vs {name2}:")
            comparison = bootstrap_comparison(preds1, preds2, true_labels,
                                             n_iterations=n_bootstrap)
            results['pairwise_bootstrap_comparison'][f"{name1}_vs_{name2}"] = comparison

            print(f"   P({name1} > {name2}):    {comparison['prob_model1_better']:.4f}")
            print(f"   P({name2} > {name1}):    {comparison['prob_model2_better']:.4f}")
            print(f"   Mean diff (bootstrap):   {comparison['mean_difference']:.4f}")
            print(f"   95% CI of difference:    [{comparison['diff_ci_lower']:.4f}, {comparison['diff_ci_upper']:.4f}]")
            print(f"   Significant at Œ±=0.05:   {'YES ‚úì' if comparison['significant_at_0.05'] else 'NO ‚úó'}")

    return results


# ============================================================================
# SECTION 5: VISUALIZATION
# ============================================================================

def visualize_statistical_results(results, models_dict, true_labels,
                                   save_path='statistical_analysis_results.png'):
    """
    Create comprehensive visualization of statistical analysis results
    """
    fig = plt.figure(figsize=(20, 14))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    fig.suptitle('Statistical Significance Analysis - Hindi Sentiment Models',
                 fontsize=18, fontweight='bold', y=0.98)

    model_names = list(results['bootstrap_ci'].keys())
    n_models = len(model_names)

    # ========================================
    # Plot 1: Confidence Intervals
    # ========================================
    ax1 = fig.add_subplot(gs[0, :2])
    means = [results['bootstrap_ci'][m]['mean'] for m in model_names]
    ci_lowers = [results['bootstrap_ci'][m]['ci_lower'] for m in model_names]
    ci_uppers = [results['bootstrap_ci'][m]['ci_upper'] for m in model_names]
    errors = [[m - l for m, l in zip(means, ci_lowers)],
              [u - m for m, u in zip(means, ci_uppers)]]

    x_pos = np.arange(len(model_names))
    colors = ['#2E86AB', '#A23B72', '#F18F01']

    for i, (x, mean, err_low, err_high) in enumerate(zip(x_pos, means, errors[0], errors[1])):
        ax1.errorbar(x, mean, yerr=[[err_low], [err_high]],
                    fmt='o', capsize=10, capthick=3,
                    markersize=12, linewidth=3,
                    color=colors[i % len(colors)], label=model_names[i])

    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(model_names, fontsize=12, fontweight='bold')
    ax1.set_ylabel('Accuracy', fontsize=13, fontweight='bold')
    ax1.set_title('Model Accuracy with 95% Bootstrap Confidence Intervals (10,000 iterations)',
                  fontsize=13, fontweight='bold', pad=15)
    ax1.grid(True, alpha=0.3, linestyle='--')
    ax1.set_ylim([min(ci_lowers) - 0.01, max(ci_uppers) + 0.01])

    # Add value labels
    for i, (mean, lower, upper) in enumerate(zip(means, ci_lowers, ci_uppers)):
        ax1.text(i, mean + 0.003, f'{mean:.4f}\n[{lower:.4f}, {upper:.4f}]',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

    # ========================================
    # Plot 2: Bootstrap Distribution Comparison
    # ========================================
    ax2 = fig.add_subplot(gs[0, 2])

    for i, name in enumerate(model_names):
        bootstrap_dist = results['bootstrap_ci'][name]['bootstrap_distribution']
        ax2.hist(bootstrap_dist, bins=40, alpha=0.6,
                label=name, color=colors[i % len(colors)], edgecolor='black')

    ax2.set_xlabel('Accuracy', fontsize=11, fontweight='bold')
    ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')
    ax2.set_title('Bootstrap Distributions', fontsize=12, fontweight='bold')
    ax2.legend(fontsize=9, loc='upper left')
    ax2.grid(True, alpha=0.3, linestyle='--')

    # ========================================
    # Plot 3: McNemar's Test p-values Heatmap
    # ========================================
    ax3 = fig.add_subplot(gs[1, 0])
    p_value_matrix = np.ones((n_models, n_models))

    for key, value in results['pairwise_mcnemar'].items():
        models = key.split('_vs_')
        i = model_names.index(models[0])
        j = model_names.index(models[1])
        p_value_matrix[i, j] = value['p_value']
        p_value_matrix[j, i] = value['p_value']

    sns.heatmap(p_value_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',
                xticklabels=model_names, yticklabels=model_names,
                vmin=0, vmax=0.1, ax=ax3, cbar_kws={'label': 'p-value'},
                linewidths=2, linecolor='black', square=True)
    ax3.set_title("McNemar's Test p-values\n(Green = significant, p<0.05)",
                  fontsize=11, fontweight='bold')

    # ========================================
    # Plot 4: t-test p-values Heatmap
    # ========================================
    ax4 = fig.add_subplot(gs[1, 1])
    t_p_value_matrix = np.ones((n_models, n_models))

    for key, value in results['pairwise_t_test'].items():
        models = key.split('_vs_')
        i = model_names.index(models[0])
        j = model_names.index(models[1])
        t_p_value_matrix[i, j] = value['p_value']
        t_p_value_matrix[j, i] = value['p_value']

    sns.heatmap(t_p_value_matrix, annot=True, fmt='.4f', cmap='RdYlGn_r',
                xticklabels=model_names, yticklabels=model_names,
                vmin=0, vmax=0.1, ax=ax4, cbar_kws={'label': 'p-value'},
                linewidths=2, linecolor='black', square=True)
    ax4.set_title("Paired t-test p-values\n(Green = significant, p<0.05)",
                  fontsize=11, fontweight='bold')

    # ========================================
    # Plot 5: Bootstrap Comparison Probabilities
    # ========================================
    ax5 = fig.add_subplot(gs[1, 2])
    comparison_pairs = []
    prob_differences = []

    for key, value in results['pairwise_bootstrap_comparison'].items():
        models = key.replace('_vs_', ' > ')
        comparison_pairs.append(models)
        prob_differences.append(value['prob_model1_better'])

    y_pos = np.arange(len(comparison_pairs))
    colors_bar = ['#2ECC71' if p > 0.5 else '#E74C3C' for p in prob_differences]

    ax5.barh(y_pos, prob_differences, color=colors_bar, alpha=0.7, edgecolor='black')
    ax5.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Equal probability')
    ax5.set_yticks(y_pos)
    ax5.set_yticklabels(comparison_pairs, fontsize=10)
    ax5.set_xlabel('Probability', fontsize=11, fontweight='bold')
    ax5.set_title('Bootstrap Comparison\nProbabilities', fontsize=11, fontweight='bold')
    ax5.set_xlim([0, 1])
    ax5.legend(fontsize=9)
    ax5.grid(True, alpha=0.3, axis='x')

    # Add probability labels
    for i, (pair, prob) in enumerate(zip(comparison_pairs, prob_differences)):
        ax5.text(prob + 0.02 if prob < 0.9 else prob - 0.02, i,
                f'{prob:.3f}', va='center', fontsize=9, fontweight='bold')

    # ========================================
    # Plot 6: Confusion Matrix Comparison
    # ========================================
    ax6 = fig.add_subplot(gs[2, :])
    ax6.axis('off')

    # Create detailed summary table
    summary_text = "COMPREHENSIVE STATISTICAL ANALYSIS SUMMARY\n"
    summary_text += "="*85 + "\n\n"

    summary_text += "1. BOOTSTRAP 95% CONFIDENCE INTERVALS (10,000 iterations):\n"
    summary_text += "-" * 85 + "\n"
    for name in model_names:
        ci = results['bootstrap_ci'][name]
        summary_text += f"   {name:12s}: {ci['mean']:.4f} [{ci['ci_lower']:.4f}, {ci['ci_upper']:.4f}]  "
        summary_text += f"(width: {ci['ci_upper']-ci['ci_lower']:.4f})\n"

    summary_text += "\n2. McNEMAR'S TEST RESULTS (œá¬≤ test on contingency tables):\n"
    summary_text += "-" * 85 + "\n"
    for key, value in results['pairwise_mcnemar'].items():
        models = key.replace('_vs_', ' vs ')
        sig_mark = "‚úì SIGNIFICANT" if value['significant'] else "‚úó Not significant"
        summary_text += f"   {models:25s}: œá¬≤={value['statistic']:6.3f}, p={value['p_value']:.4f}  {sig_mark}\n"

    summary_text += "\n3. PAIRED T-TEST RESULTS:\n"
    summary_text += "-" * 85 + "\n"
    for key, value in results['pairwise_t_test'].items():
        models = key.replace('_vs_', ' vs ')
        sig_mark = "‚úì SIGNIFICANT" if value['significant'] else "‚úó Not significant"
        summary_text += f"   {models:25s}: t={value['t_statistic']:6.3f}, p={value['p_value']:.4f}  {sig_mark}\n"
        summary_text += f"   {'':27s}  Cohen's d={value['cohens_d']:.4f}, Mean diff={value['mean_difference']:.4f}\n"

    summary_text += "\n4. INTERPRETATION & CONCLUSION:\n"
    summary_text += "-" * 85 + "\n"

    # Determine if differences are significant
    any_significant = any(v['significant'] for v in results['pairwise_mcnemar'].values())

    if not any_significant:
        summary_text += "   ‚ö† NO STATISTICALLY SIGNIFICANT DIFFERENCES detected between models (Œ±=0.05)\n"
        summary_text += "   ‚ö† Performance differences are within statistical noise\n"
        summary_text += "   ‚ö† Claims of superiority should be tempered or avoided\n"
    else:
        summary_text += "   ‚úì Statistically significant differences detected for some model pairs\n"
        summary_text += "   ‚úì Review individual test results above for specific comparisons\n"

    summary_text += "\n" + "="*85 + "\n"
    summary_text += "NOTE: All tests use Œ±=0.05 significance level. Bootstrap uses 10,000 iterations.\n"
    summary_text += "For publication, report both effect sizes and statistical significance."

    ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes,
             fontsize=9, verticalalignment='top', family='monospace',
             bbox=dict(boxstyle='round', facecolor='#FFF9E3', alpha=0.8, pad=1))

    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"\n‚úÖ Visualization saved to: {save_path}")
    plt.show()


# ============================================================================
# SECTION 6: GENERATE PUBLICATION TABLE
# ============================================================================

def generate_publication_table(results, models_dict, true_labels):
    """
    Generate publication-ready tables for manuscript
    """
    print(f"\n{'='*90}")
    print("TABLE 1: Model Performance with Statistical Validation")
    print(f"{'='*90}\n")

    # Create main results table
    table_data = []

    for model_name, preds in models_dict.items():
        ci_data = results['bootstrap_ci'][model_name]
        acc = accuracy_score(true_labels, preds)

        # Get per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(
            true_labels, preds, average=None, zero_division=0
        )

        # Get weighted averages
        precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(
            true_labels, preds, average='weighted', zero_division=0
        )

        table_data.append({
            'Model': model_name,
            'Accuracy': f"{acc:.4f}",
            '95% CI': f"[{ci_data['ci_lower']:.4f}, {ci_data['ci_upper']:.4f}]",
            'CI Width': f"{ci_data['ci_upper'] - ci_data['ci_lower']:.4f}",
            'Precision': f"{precision_w:.4f}",
            'Recall': f"{recall_w:.4f}",
            'F1-Score': f"{f1_w:.4f}"
        })

    df_results = pd.DataFrame(table_data)
    print(df_results.to_string(index=False))
    print(f"\n{'='*90}")

    # Create pairwise comparison table
    print(f"\n{'='*90}")
    print("TABLE 2: Pairwise Statistical Significance Tests")
    print(f"{'='*90}\n")

    comparison_data = []
    for key in results['pairwise_mcnemar'].keys():
        models = key.split('_vs_')
        mcnemar_val = results['pairwise_mcnemar'][key]
        t_val = results['pairwise_t_test'][key]
        boot_val = results['pairwise_bootstrap_comparison'][key]

        comparison_data.append({
            'Comparison': f"{models[0]} vs {models[1]}",
            'Acc Diff': f"{t_val['mean_difference']:.4f}",
            "McNemar œá¬≤": f"{mcnemar_val['statistic']:.4f}",
            'McNemar p': f"{mcnemar_val['p_value']:.4f}",
            't-test p': f"{t_val['p_value']:.4f}",
            "Cohen's d": f"{t_val['cohens_d']:.4f}",
            'Bootstrap p': f"{boot_val['prob_model1_better']:.4f}",
            'Significant': '‚úì' if mcnemar_val['significant'] else '‚úó'
        })

    df_comparison = pd.DataFrame(comparison_data)
    print(df_comparison.to_string(index=False))
    print("\n* ‚úì indicates p < 0.05 (statistically significant difference)")
    print("* Cohen's d: <0.2=negligible, 0.2-0.5=small, 0.5-0.8=medium, >0.8=large")
    print(f"{'='*90}\n")

    # Create contingency table details
    print(f"\n{'='*90}")
    print("TABLE 3: McNemar's Test Contingency Tables")
    print(f"{'='*90}\n")

    for key, mcnemar_val in results['pairwise_mcnemar'].items():
        models = key.split('_vs_')
        print(f"\n{models[0]} vs {models[1]}:")
        print(f"   Both Correct:         {mcnemar_val['both_correct']}")
        print(f"   Only {models[0]} Correct: {mcnemar_val['model1_only_correct']}")
        print(f"   Only {models[1]} Correct: {mcnemar_val['model2_only_correct']}")
        print(f"   Both Wrong:           {mcnemar_val['both_wrong']}")
        if mcnemar_val['odds_ratio'] != float('inf'):
            print(f"   Odds Ratio:           {mcnemar_val['odds_ratio']:.4f}")

    print(f"\n{'='*90}")

    # Save to CSV
    df_results.to_csv('statistical_results_main.csv', index=False)
    df_comparison.to_csv('statistical_results_pairwise.csv', index=False)

    # Create detailed report
    with open('statistical_analysis_report.txt', 'w') as f:
        f.write("="*90 + "\n")
        f.write("STATISTICAL SIGNIFICANCE ANALYSIS REPORT\n")
        f.write("Hindi Sentiment Analysis Models\n")
        f.write("="*90 + "\n\n")

        f.write("METHODOLOGY:\n")
        f.write("-" * 90 + "\n")
        f.write("1. Bootstrap Confidence Intervals: 10,000 iterations with replacement sampling\n")
        f.write("2. McNemar's Test: Chi-squared test on contingency tables (paired nominal data)\n")
        f.write("3. Paired t-test: Parametric test comparing per-sample correctness\n")
        f.write("4. Bootstrap Comparison: Non-parametric probability estimation\n")
        f.write("5. Significance Level: Œ± = 0.05 for all tests\n\n")

        f.write("="*90 + "\n")
        f.write("TABLE 1: Model Performance\n")
        f.write("="*90 + "\n")
        f.write(df_results.to_string(index=False))
        f.write("\n\n")

        f.write("="*90 + "\n")
        f.write("TABLE 2: Pairwise Comparisons\n")
        f.write("="*90 + "\n")
        f.write(df_comparison.to_string(index=False))
        f.write("\n\n")

        f.write("="*90 + "\n")
        f.write("INTERPRETATION GUIDELINES:\n")
        f.write("="*90 + "\n")
        f.write("‚Ä¢ McNemar's Test: Tests if two models have significantly different error rates\n")
        f.write("‚Ä¢ Paired t-test: Tests if mean per-sample correctness differs significantly\n")
        f.write("‚Ä¢ Cohen's d: Measures effect size (practical significance vs statistical)\n")
        f.write("‚Ä¢ Bootstrap CI: Non-parametric estimate of performance uncertainty\n")
        f.write("‚Ä¢ Bootstrap p: Probability one model outperforms another\n\n")

        f.write("CONCLUSION:\n")
        f.write("-" * 90 + "\n")

        # Analyze results
        any_significant = any(v['significant'] for v in results['pairwise_mcnemar'].values())

        if not any_significant:
            f.write("‚ö† IMPORTANT: No statistically significant differences detected between models.\n")
            f.write("The observed performance differences (0.5-0.8%) fall within statistical noise.\n")
            f.write("RECOMMENDATION: Avoid claims of 'significant outperformance' without qualification.\n")
            f.write("Instead use phrasing like:\n")
            f.write('  - "marginally outperforms" or "shows slight improvement"\n')
            f.write('  - "achieves comparable performance"\n')
            f.write('  - "demonstrates competitive results"\n')
        else:
            f.write("‚úì Statistically significant differences detected for some model pairs.\n")
            f.write("Review individual test results and effect sizes for proper interpretation.\n")

        f.write("\n" + "="*90 + "\n")

    print("‚úÖ Tables saved to:")
    print("   - statistical_results_main.csv")
    print("   - statistical_results_pairwise.csv")
    print("   - statistical_analysis_report.txt")

    return df_results, df_comparison


# ============================================================================
# SECTION 7: LATEX TABLE GENERATOR
# ============================================================================

def generate_latex_tables(results, models_dict, true_labels):
    """
    Generate LaTeX formatted tables for publication
    """
    latex_content = ""

    # Table 1: Main Results
    latex_content += "% Table 1: Model Performance with Statistical Validation\n"
    latex_content += "\\begin{table}[htbp]\n"
    latex_content += "\\centering\n"
    latex_content += "\\caption{Performance Comparison with Bootstrap Confidence Intervals (10,000 iterations)}\n"
    latex_content += "\\label{tab:performance}\n"
    latex_content += "\\begin{tabular}{lccccc}\n"
    latex_content += "\\hline\n"
    latex_content += "\\textbf{Model} & \\textbf{Accuracy} & \\textbf{95\\% CI} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1-Score} \\\\\n"
    latex_content += "\\hline\n"

    for model_name, preds in models_dict.items():
        ci_data = results['bootstrap_ci'][model_name]
        acc = accuracy_score(true_labels, preds)
        precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(
            true_labels, preds, average='weighted', zero_division=0
        )

        latex_content += f"{model_name} & {acc:.4f} & "
        latex_content += f"[{ci_data['ci_lower']:.4f}, {ci_data['ci_upper']:.4f}] & "
        latex_content += f"{precision_w:.4f} & {recall_w:.4f} & {f1_w:.4f} \\\\\n"

    latex_content += "\\hline\n"
    latex_content += "\\end{tabular}\n"
    latex_content += "\\end{table}\n\n"

    # Table 2: Statistical Tests
    latex_content += "% Table 2: Pairwise Statistical Significance Tests\n"
    latex_content += "\\begin{table}[htbp]\n"
    latex_content += "\\centering\n"
    latex_content += "\\caption{Pairwise Statistical Significance Analysis}\n"
    latex_content += "\\label{tab:significance}\n"
    latex_content += "\\begin{tabular}{lcccc}\n"
    latex_content += "\\hline\n"
    latex_content += "\\textbf{Comparison} & \\textbf{McNemar $\\chi^2$} & \\textbf{$p$-value} & \\textbf{Cohen's $d$} & \\textbf{Significant} \\\\\n"
    latex_content += "\\hline\n"

    for key in results['pairwise_mcnemar'].keys():
        models = key.split('_vs_')
        mcnemar_val = results['pairwise_mcnemar'][key]
        t_val = results['pairwise_t_test'][key]

        comparison_str = f"{models[0]} vs {models[1]}"
        sig_mark = "Yes" if mcnemar_val['significant'] else "No"

        latex_content += f"{comparison_str} & {mcnemar_val['statistic']:.3f} & "
        latex_content += f"{mcnemar_val['p_value']:.4f} & {t_val['cohens_d']:.3f} & {sig_mark} \\\\\n"

    latex_content += "\\hline\n"
    latex_content += "\\multicolumn{5}{l}{\\textit{Note:} Significance level $\\alpha = 0.05$. Cohen's $d$: $<0.2$ negligible, $0.2$-$0.5$ small, $0.5$-$0.8$ medium, $>0.8$ large.} \\\\\n"
    latex_content += "\\end{tabular}\n"
    latex_content += "\\end{table}\n\n"

    # Save LaTeX tables
    with open('statistical_tables_latex.tex', 'w') as f:
        f.write(latex_content)

    print("\n‚úÖ LaTeX tables saved to: statistical_tables_latex.tex")

    return latex_content


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Main execution function
    """
    print("\n" + "="*80)
    print("STATISTICAL SIGNIFICANCE ANALYSIS FOR HINDI SENTIMENT ANALYSIS")
    print("Addressing Reviewer Feedback on Statistical Validation")
    print("="*80)

    # Define file paths
    file_paths = {
        'mBERT': 'test_predictions_new2_mbert.csv',
        'XLM-R': 'test_predictions_new2_xmlr.csv',
        'Proposed': 'test_predictions_new2_Proposed.csv'
    }

    # ========================================
    # Step 1: Load all data
    # ========================================
    print("\n" + "="*80)
    print("STEP 1: LOADING DATA")
    print("="*80)

    predictions_dict = {}
    labels = None

    for model_name, file_path in file_paths.items():
        df = load_predictions(file_path, model_name)

        if df is not None:
            preds, lbls, pred_col, label_col = extract_predictions_and_labels(df, model_name)

            if preds is not None and lbls is not None:
                predictions_dict[model_name] = preds

                # Use labels from first model (all should be same)
                if labels is None:
                    labels = lbls
                else:
                    # Verify labels are consistent
                    if not np.array_equal(labels, lbls):
                        print(f"\n‚ö†Ô∏è  WARNING: Labels differ between models!")
                        print(f"   This suggests different test sets were used!")
                        return

    if len(predictions_dict) == 0:
        print("\n‚ùå ERROR: No predictions loaded successfully")
        return

    if labels is None:
        print("\n‚ùå ERROR: No labels found")
        return

    print(f"\n‚úÖ Successfully loaded predictions for {len(predictions_dict)} models")

    # ========================================
    # Step 2: Verify data alignment
    # ========================================
    print("\n" + "="*80)
    print("STEP 2: VERIFYING DATA ALIGNMENT")
    print("="*80)

    is_valid = verify_data_alignment(predictions_dict, labels)

    if not is_valid:
        print("\n‚ùå Data validation failed. Please fix errors before proceeding.")
        return

    # ========================================
    # Step 3: Comprehensive Statistical Analysis
    # ========================================
    print("\n" + "="*80)
    print("STEP 3: COMPREHENSIVE STATISTICAL ANALYSIS")
    print("="*80)

    results = comprehensive_statistical_analysis(predictions_dict, labels,
                                                 n_bootstrap=10000)

    # ========================================
    # Step 4: Generate Visualizations
    # ========================================
    print("\n" + "="*80)
    print("STEP 4: GENERATING VISUALIZATIONS")
    print("="*80)

    visualize_statistical_results(results, predictions_dict, labels,
                                  save_path='statistical_analysis_results.png')

    # ========================================
    # Step 5: Generate Tables
    # ========================================
    print("\n" + "="*80)
    print("STEP 5: GENERATING TABLES")
    print("="*80)

    df_results, df_comparison = generate_publication_table(results, predictions_dict, labels)

    # ========================================
    # Step 6: Generate LaTeX Tables
    # ========================================
    print("\n" + "="*80)
    print("STEP 6: GENERATING LATEX TABLES")
    print("="*80)

    latex_tables = generate_latex_tables(results, predictions_dict, labels)

    # ========================================
    # Step 7: Final Summary
    # ========================================
    print("\n" + "="*80)
    print("ANALYSIS COMPLETE - SUMMARY AND RECOMMENDATIONS")
    print("="*80)

    print("\nüìä Generated Files:")
    print("   1. statistical_analysis_results.png - Comprehensive visualization")
    print("   2. statistical_results_main.csv - Main performance table")
    print("   3. statistical_results_pairwise.csv - Pairwise comparison table")
    print("   4. statistical_analysis_report.txt - Detailed text report")
    print("   5. statistical_tables_latex.tex - LaTeX tables for publication")

    print("\nüí° Key Findings:")
    any_significant = any(v['significant'] for v in results['pairwise_mcnemar'].values())

    if not any_significant:
        print("   ‚ö†Ô∏è  NO statistically significant differences detected (Œ±=0.05)")
        print("   ‚ö†Ô∏è  Performance differences (0.5-0.8%) are within statistical noise")
        print("\nüìù RECOMMENDATION FOR MANUSCRIPT:")
        print('   Replace: "significantly surpasses transformer-based architectures"')
        print('   With: "achieves competitive/comparable performance" or')
        print('         "shows marginal improvements over baseline models"')
    else:
        print("   ‚úÖ Statistically significant differences detected for some pairs")
        print("   ‚úÖ Review detailed results to identify which comparisons are significant")


    print("\n‚úÖ Analysis complete!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()