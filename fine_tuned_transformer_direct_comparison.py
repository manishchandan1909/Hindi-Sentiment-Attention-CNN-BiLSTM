# -*- coding: utf-8 -*-
"""Fine_Tuned_Transformer_Direct_Comparison.ipynb

Automatically generated by Colab.
"""

!pip install indic-nlp-library

# download the resource
!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git

# download the repo
!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git


import sys
from indicnlp import common

# The path to the local git repo for Indic NLP library
INDIC_NLP_LIB_HOME=r"indic_nlp_library"

# The path to the local git repo for Indic NLP Resources
INDIC_NLP_RESOURCES=r"indic_nlp_resources"

# Add library to Python path
sys.path.append(r'{}\src'.format(INDIC_NLP_LIB_HOME))

# Set environment variable for resources folder
common.set_resources_path(INDIC_NLP_RESOURCES)

# Mount Google Drive
drive.mount('/content/gdrive')

from google.colab import drive
import re
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import nltk
from tqdm import tqdm
import matplotlib.patheffects as path_effects

from transformers import BertTokenizer, BertModel
from transformers import XLMRobertaTokenizer, XLMRobertaModel
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory

# Download NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

"""# ***mBERT***"""

# ============================================
# CONFIGURATION WITH OPTIMIZED HYPERPARAMETERS
# ============================================

class Config:
    """Configuration parameters with optimized hyperparameters from tuning"""
    # Paths
    DATASET_PATH = 'combined_dataset.csv'

    # Model parameters - OPTIMIZED HYPERPARAMETERS
    MODEL_NAME = 'bert-base-multilingual-cased'
    MAX_LEN = 256
    BATCH_SIZE = 16
    LEARNING_RATE = 1.569639638866115e-05
    DROPOUT = 0.20853961270955837
    WEIGHT_DECAY = 0.003063462210622081
    EPOCHS = 40
    OPTIMIZER_TYPE = 'adamw'

    # Early stopping parameters
    PATIENCE = 3  # Number of epochs to wait for improvement
    MIN_DELTA = 0.001  # Minimum change to qualify as improvement

    # Random seed for reproducibility
    RANDOM_SEED = 42

# Set random seeds
torch.manual_seed(Config.RANDOM_SEED)
np.random.seed(Config.RANDOM_SEED)

print("="*70)
print("OPTIMIZED HYPERPARAMETERS FROM TUNING")
print("="*70)
print(f"Learning Rate:        {Config.LEARNING_RATE:.2e}")
print(f"Batch Size:           {Config.BATCH_SIZE}")
print(f"Max Length:           {Config.MAX_LEN}")
print(f"Dropout:              {Config.DROPOUT:.4f}")
print(f"Weight Decay:         {Config.WEIGHT_DECAY:.6f}")
print(f"Epochs:               {Config.EPOCHS}")
print(f"Optimizer:            {Config.OPTIMIZER_TYPE.upper()}")
print("="*70 + "\n")

# ============================================
# EARLY STOPPING CLASS
# ============================================

class EarlyStopping:
    """Early stopping to stop training when validation loss doesn't improve"""

    def __init__(self, patience=3, min_delta=0.001, verbose=True):
        """
        Args:
            patience (int): How many epochs to wait after last improvement
            min_delta (float): Minimum change to qualify as improvement
            verbose (bool): If True, prints messages
        """
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_epoch = 0

    def __call__(self, val_loss, epoch):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_epoch = epoch
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f'\n⚠ Early stopping triggered! Best validation loss: {self.best_loss:.4f} at epoch {self.best_epoch}')
        else:
            self.best_loss = val_loss
            self.best_epoch = epoch
            self.counter = 0

# ============================================
# DATA LOADING AND PREPROCESSING
# ============================================

def load_and_preprocess_data(dataset_path):
    """Load and perform initial preprocessing on the dataset"""
    df = pd.read_csv(dataset_path)

    print("First few rows of the dataset:")
    print(df.head())

    # Handle missing values
    print("\nHandling missing values...")
    initial_size = len(df)
    df = df.dropna()
    print(f"Removed {initial_size - len(df)} rows with missing values")

    # Display class distribution
    print("\nClass distribution:")
    print(df['polarity'].value_counts())

    return df

# Initialize Indic NLP normalizer
normalizer_factory = IndicNormalizerFactory()
normalizer = normalizer_factory.get_normalizer('hi')

def process_text(text):
    """Clean and normalize Hindi text"""
    text = normalizer.normalize(text)
    text = text.lower()
    text = re.sub(r'((www\.[^\s]+)|(https?://[^\s]+))', '', text)
    text = re.sub(r'@[^\s]+', '', text)
    text = re.sub(r'#([^\s]+)', r'\1', text)
    text = re.sub(r'[^\u0900-\u097F\s]', '', text)
    text = re.sub(r'[.,!?:;\-\'"\/\(\)[\]\{\}]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.strip('\'"').replace('।', '')
    return text

def preprocess_dataframe(df):
    """Apply all preprocessing steps to the dataframe"""
    print("\nPreprocessing text...")
    df['text'] = df['text'].apply(process_text)

    print("\nPreprocessed data sample:")
    print(df[['text', 'polarity']].head())

    return df

# ============================================
# VISUALIZATION
# ============================================

def plot_polarity_distribution(df):
    """Create an enhanced polarity distribution plot"""
    polarity_counts = df['polarity'].value_counts().sort_index()

    sns.set_theme(style="whitegrid")
    colors = ['red', 'green']

    fig, ax = plt.subplots(figsize=(6.5, 5))
    bars = ax.bar(polarity_counts.index, polarity_counts.values, color=colors,
                  edgecolor='#2c3e50', linewidth=1.0, width=0.8, alpha=0.90)

    patterns = ['/', '\\']
    for bar, pattern in zip(bars, patterns):
        bar.set_hatch(pattern)

    ax.yaxis.grid(True, linestyle='--', color='gray', alpha=0.3)
    ax.xaxis.grid(False)
    ax.set_axisbelow(True)

    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, yval + max(polarity_counts.values)*0.02,
                f'{int(yval)}', ha='center', va='bottom', fontsize=12, fontweight='bold',
                color='#2c3e50', path_effects=[path_effects.withStroke(linewidth=1, foreground="white")])

    ax.set_xlabel('Polarity', fontsize=15, fontweight='bold', color='#34495e')
    ax.set_ylabel('Count', fontsize=15, fontweight='bold', color='#34495e')
    ax.set_title('Polarity Distribution', fontsize=18, fontweight='bold', color='#2c3e50')
    ax.tick_params(axis='x', labelsize=12)
    ax.tick_params(axis='y', labelsize=10)
    ax.set_ylim(0, max(polarity_counts.values)*1.15)

    plt.tight_layout()
    plt.show()

# ============================================
# DATASET CLASS
# ============================================

class SentimentDataset(Dataset):
    """Custom Dataset for sentiment analysis"""

    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ============================================
# OPTIMIZED MODEL DEFINITION WITH LAYER FREEZING
# ============================================

class mBERT_Classifier(nn.Module):
    """mBERT-based sentiment classifier with optimized hyperparameters"""

    def __init__(self, model_name, num_classes, dropout=0.3):
        super(mBERT_Classifier, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.bert_hidden_size = self.bert.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.bert_hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = bert_output.pooler_output
        cls_output = self.dropout(cls_output)
        logits = self.fc(cls_output)
        return logits

# ============================================
# TRAINING AND EVALUATION FUNCTIONS
# ============================================

def train_epoch(model, data_loader, criterion, optimizer, device, scaler):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(data_loader, desc="Training")

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        with autocast():
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        _, preds = torch.max(logits, dim=1)
        correct_predictions += torch.sum(preds == labels).item()
        total_predictions += labels.size(0)
        running_loss += loss.item()

        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{correct_predictions / total_predictions:.4f}'
        })

    epoch_loss = running_loss / len(data_loader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

def eval_model(model, data_loader, criterion, device):
    """Evaluate the model"""
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0
    all_preds = []
    all_labels = []

    progress_bar = tqdm(data_loader, desc="Validation")

    with torch.no_grad():
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            with autocast():
                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)

            _, preds = torch.max(logits, dim=1)
            correct_predictions += torch.sum(preds == labels).item()
            total_predictions += labels.size(0)
            running_loss += loss.item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{correct_predictions / total_predictions:.4f}'
            })

    epoch_loss = running_loss / len(data_loader)
    epoch_acc = correct_predictions / total_predictions
    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')

    return epoch_loss, epoch_acc, epoch_f1

def get_predictions(model, data_loader, device):
    """Get predictions for the entire dataset"""
    model.eval()
    all_predictions = []
    all_labels = []

    progress_bar = tqdm(data_loader, desc="Predicting")

    with torch.no_grad():
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            with autocast():
                logits = model(input_ids, attention_mask)

            _, preds = torch.max(logits, dim=1)
            all_predictions.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_predictions), np.array(all_labels)

# ============================================
# VISUALIZATION FUNCTIONS
# ============================================

def plot_confusion_matrix(y_true, y_pred, class_labels):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_labels)))

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels,
                cbar_kws={'label': 'Count'})
    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')
    plt.ylabel('True Label', fontsize=12, fontweight='bold')
    plt.title('Confusion Matrix - Optimized mBERT', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    """Plot training history with F1 score"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Plot Loss
    axes[0].plot(history['train_loss'], label='Training Loss', marker='o', linewidth=2)
    axes[0].plot(history['val_loss'], label='Validation Loss', marker='s', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')
    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    axes[0].legend(loc='best', fontsize=10)
    axes[0].grid(True, alpha=0.3)

    # Plot Accuracy
    axes[1].plot(history['train_acc'], label='Training Accuracy', marker='o', linewidth=2)
    axes[1].plot(history['val_acc'], label='Validation Accuracy', marker='s', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    axes[1].legend(loc='best', fontsize=10)
    axes[1].grid(True, alpha=0.3)

    # Plot F1 Score
    axes[2].plot(history['val_f1'], label='Validation F1', marker='D', linewidth=2, color='green')
    axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[2].set_ylabel('F1 Score', fontsize=12, fontweight='bold')
    axes[2].set_title('Validation F1 Score', fontsize=14, fontweight='bold')
    axes[2].legend(loc='best', fontsize=10)
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ============================================
# MAIN EXECUTION
# ============================================

def main():
    """Main execution function with optimized hyperparameters"""

    # Load and preprocess data
    print("\n" + "="*70)
    print("DATA LOADING AND PREPROCESSING")
    print("="*70)

    df = load_and_preprocess_data(Config.DATASET_PATH)
    plot_polarity_distribution(df)
    df = preprocess_dataframe(df)

    # Label encoding
    print("\nEncoding labels...")
    label_encoder = LabelEncoder()
    df['polarity'] = label_encoder.fit_transform(df['polarity'])
    class_labels = label_encoder.classes_

    print("\nEncoded Labels:")
    for label, encoded_value in zip(class_labels, label_encoder.transform(class_labels)):
        print(f"Polarity: {label} → Encoded: {encoded_value}")

    # Train-test split
    print(f"\n{'='*70}")
    print("DATASET SPLIT")
    print("="*70)

    X = df['text'].values
    y = df['polarity'].values

    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=Config.RANDOM_SEED
    )

    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=Config.RANDOM_SEED
    )

    print(f"Training samples: {len(X_train)}")
    print(f"Validation samples: {len(X_val)}")
    print(f"Test samples: {len(X_test)}")

    # Create datasets and dataloaders with optimized parameters
    print("\nLoading tokenizer and creating datasets...")
    tokenizer = BertTokenizer.from_pretrained(Config.MODEL_NAME)

    train_dataset = SentimentDataset(X_train, y_train, tokenizer, Config.MAX_LEN)
    val_dataset = SentimentDataset(X_val, y_val, tokenizer, Config.MAX_LEN)
    test_dataset = SentimentDataset(X_test, y_test, tokenizer, Config.MAX_LEN)

    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)

    print(f"Training batches: {len(train_loader)}")
    print(f"Validation batches: {len(val_loader)}")
    print(f"Test batches: {len(test_loader)}")

    # Initialize model with optimized parameters
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*70}")
    print(f"DEVICE: {device}")
    print(f"{'='*70}")

    num_classes = len(class_labels)
    model = mBERT_Classifier(
        Config.MODEL_NAME,
        num_classes,
        dropout=Config.DROPOUT
    ).to(device)

    print(f"\nModel: {Config.MODEL_NAME}")
    print(f"Number of classes: {num_classes}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Training setup with optimized optimizer
    criterion = nn.CrossEntropyLoss()

    if Config.OPTIMIZER_TYPE == 'adamw':
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=Config.LEARNING_RATE,
            weight_decay=Config.WEIGHT_DECAY
        )
        print(f"\nOptimizer: AdamW")
    else:
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=Config.LEARNING_RATE,
            weight_decay=Config.WEIGHT_DECAY
        )
        print(f"\nOptimizer: Adam")

    print(f"Learning Rate: {Config.LEARNING_RATE:.2e}")
    print(f"Weight Decay: {Config.WEIGHT_DECAY:.6f}")

    scaler = GradScaler()
    early_stopping = EarlyStopping(patience=Config.PATIENCE, min_delta=Config.MIN_DELTA)

    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': []
    }

    # Training loop
    print("\n" + "="*70)
    print("STARTING TRAINING WITH OPTIMIZED HYPERPARAMETERS")
    print("="*70)
    print(f"Patience: {Config.PATIENCE} epochs")
    print(f"Min Delta: {Config.MIN_DELTA}")
    print("\n")

    best_val_f1 = 0.0

    for epoch in range(Config.EPOCHS):
        print(f"\nEpoch {epoch + 1}/{Config.EPOCHS}")
        print("-" * 70)

        # Train
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, scaler
        )

        # Validate
        val_loss, val_acc, val_f1 = eval_model(model, val_loader, criterion, device)

        # Store history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)

        # Print epoch summary
        print(f"\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

        # Track best F1
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1

        # Early stopping check
        early_stopping(val_loss, epoch + 1)
        if early_stopping.early_stop:
            print(f"\n{'='*70}")
            print(f"Training stopped at epoch {epoch + 1}")
            print(f"Best model from epoch {early_stopping.best_epoch}")
            print(f"{'='*70}\n")
            break

    print("\n" + "="*70)
    print("TRAINING COMPLETED")
    print("="*70 + "\n")

    # Get predictions on test set
    print("Evaluating on test set...")
    y_pred, y_true = get_predictions(model, test_loader, device)

    test_accuracy = accuracy_score(y_true, y_pred)
    test_f1 = f1_score(y_true, y_pred, average='weighted')

    print(f"\n{'='*70}")
    print("FINAL TEST RESULTS WITH OPTIMIZED HYPERPARAMETERS")
    print(f"{'='*70}")
    print(f"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print(f"Test F1 Score: {test_f1:.4f}")
    print(f"{'='*70}\n")

    # Classification report
    print("\n" + "="*70)
    print("CLASSIFICATION REPORT")
    print("="*70 + "\n")

    target_names = [str(label) for label in class_labels]
    print(classification_report(y_true, y_pred, target_names=target_names))

    # Visualizations
    plot_confusion_matrix(y_true, y_pred, class_labels)
    plot_training_history(history)

    # Summary of optimized configuration
    print("\n" + "="*70)
    print("TRAINING SUMMARY - OPTIMIZED MODEL")
    print("="*70)
    print(f"Model: {Config.MODEL_NAME}")
    print(f"Optimized Hyperparameters:")
    print(f"  - Learning Rate:       {Config.LEARNING_RATE:.2e}")
    print(f"  - Batch Size:          {Config.BATCH_SIZE}")
    print(f"  - Max Length:          {Config.MAX_LEN}")
    print(f"  - Dropout:             {Config.DROPOUT:.4f}")
    print(f"  - Weight Decay:        {Config.WEIGHT_DECAY:.6f}")
    print(f"  - Optimizer:           {Config.OPTIMIZER_TYPE.upper()}")
    print(f"\nFinal Results:")
    print(f"  - Best Val F1:         {best_val_f1:.4f}")
    print(f"  - Test Accuracy:       {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print(f"  - Test F1 Score:       {test_f1:.4f}")
    print(f"  - Total Epochs:        {len(history['train_loss'])}")
    print("="*70)

    print("\n" + "="*70)
    print("ALL TASKS COMPLETED SUCCESSFULLY")
    print("="*70)

    return model, history, test_accuracy, test_f1

# Run main function
if __name__ == "__main__":
    model, history, test_acc, test_f1 = main()

"""# ***XLM-RoBERTa***"""

# ============================================
# CONFIGURATION WITH OPTIMIZED HYPERPARAMETERS
# ============================================

class Config:
    """Configuration parameters with optimized hyperparameters from tuning"""
    # Paths
    DATASET_PATH = 'combined_dataset.csv'

    # Model parameters - OPTIMIZED HYPERPARAMETERS FOR XLM-RoBERTa
    MODEL_NAME = 'xlm-roberta-base'
    MAX_LEN = 256
    BATCH_SIZE = 32
    LEARNING_RATE = 1.1715937392307063e-06
    DROPOUT = 0.17394178221021084
    WEIGHT_DECAY = 0.008105016126411584
    EPOCHS = 40
    OPTIMIZER_TYPE = 'adamw'

    # Early stopping parameters
    PATIENCE = 3  # Number of epochs to wait for improvement
    MIN_DELTA = 0.001  # Minimum change to qualify as improvement

    # Random seed for reproducibility
    RANDOM_SEED = 42

# Set random seeds
torch.manual_seed(Config.RANDOM_SEED)
np.random.seed(Config.RANDOM_SEED)

print("="*70)
print("OPTIMIZED HYPERPARAMETERS FOR XLM-RoBERTa")
print("="*70)
print(f"Model:                {Config.MODEL_NAME}")
print(f"Learning Rate:        {Config.LEARNING_RATE:.2e}")
print(f"Batch Size:           {Config.BATCH_SIZE}")
print(f"Max Length:           {Config.MAX_LEN}")
print(f"Dropout:              {Config.DROPOUT:.4f}")
print(f"Weight Decay:         {Config.WEIGHT_DECAY:.6f}")
print(f"Epochs:               {Config.EPOCHS}")
print(f"Optimizer:            {Config.OPTIMIZER_TYPE.upper()}")
print("="*70 + "\n")

# ============================================
# EARLY STOPPING CLASS
# ============================================

class EarlyStopping:
    """Early stopping to stop training when validation loss doesn't improve"""

    def __init__(self, patience=3, min_delta=0.001, verbose=True):
        """
        Args:
            patience (int): How many epochs to wait after last improvement
            min_delta (float): Minimum change to qualify as improvement
            verbose (bool): If True, prints messages
        """
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_epoch = 0

    def __call__(self, val_loss, epoch):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_epoch = epoch
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f'\n⚠ Early stopping triggered! Best validation loss: {self.best_loss:.4f} at epoch {self.best_epoch}')
        else:
            self.best_loss = val_loss
            self.best_epoch = epoch
            self.counter = 0

# ============================================
# DATA LOADING AND PREPROCESSING
# ============================================

def load_and_preprocess_data(dataset_path):
    """Load and perform initial preprocessing on the dataset"""
    df = pd.read_csv(dataset_path)

    print("First few rows of the dataset:")
    print(df.head())

    # Handle missing values
    print("\nHandling missing values...")
    initial_size = len(df)
    df = df.dropna()
    print(f"Removed {initial_size - len(df)} rows with missing values")

    # Display class distribution
    print("\nClass distribution:")
    print(df['polarity'].value_counts())

    return df

# Initialize Indic NLP normalizer
normalizer_factory = IndicNormalizerFactory()
normalizer = normalizer_factory.get_normalizer('hi')

def process_text(text):
    """Clean and normalize Hindi text"""
    text = normalizer.normalize(text)
    text = text.lower()
    text = re.sub(r'((www\.[^\s]+)|(https?://[^\s]+))', '', text)
    text = re.sub(r'@[^\s]+', '', text)
    text = re.sub(r'#([^\s]+)', r'\1', text)
    text = re.sub(r'[^\u0900-\u097F\s]', '', text)
    text = re.sub(r'[.,!?:;\-\'"\/\(\)\[\]\{\}]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.strip('\'"').replace('।', '')
    return text

def preprocess_dataframe(df):
    """Apply all preprocessing steps to the dataframe"""
    print("\nPreprocessing text...")
    df['text'] = df['text'].apply(process_text)

    print("\nPreprocessed data sample:")
    print(df[['text', 'polarity']].head())

    return df

# ============================================
# VISUALIZATION
# ============================================

def plot_polarity_distribution(df):
    """Create an enhanced polarity distribution plot"""
    polarity_counts = df['polarity'].value_counts().sort_index()

    sns.set_theme(style="whitegrid")
    colors = ['red', 'green']

    fig, ax = plt.subplots(figsize=(6.5, 5))
    bars = ax.bar(polarity_counts.index, polarity_counts.values, color=colors,
                  edgecolor='#2c3e50', linewidth=1.0, width=0.8, alpha=0.90)

    patterns = ['/', '\\']
    for bar, pattern in zip(bars, patterns):
        bar.set_hatch(pattern)

    ax.yaxis.grid(True, linestyle='--', color='gray', alpha=0.3)
    ax.xaxis.grid(False)
    ax.set_axisbelow(True)

    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, yval + max(polarity_counts.values)*0.02,
                f'{int(yval)}', ha='center', va='bottom', fontsize=12, fontweight='bold',
                color='#2c3e50', path_effects=[path_effects.withStroke(linewidth=1, foreground="white")])

    ax.set_xlabel('Polarity', fontsize=15, fontweight='bold', color='#34495e')
    ax.set_ylabel('Count', fontsize=15, fontweight='bold', color='#34495e')
    ax.set_title('Polarity Distribution', fontsize=18, fontweight='bold', color='#2c3e50')
    ax.tick_params(axis='x', labelsize=12)
    ax.tick_params(axis='y', labelsize=10)
    ax.set_ylim(0, max(polarity_counts.values)*1.15)

    plt.tight_layout()
    plt.show()

# ============================================
# DATASET CLASS
# ============================================

class SentimentDataset(Dataset):
    """Custom Dataset for sentiment analysis"""

    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ============================================
# XLM-RoBERTa MODEL DEFINITION
# ============================================

class XLMRoberta_Classifier(nn.Module):
    """XLM-RoBERTa-based sentiment classifier with optimized hyperparameters"""

    def __init__(self, model_name, num_classes, dropout=0.3):
        super(XLMRoberta_Classifier, self).__init__()
        self.xlmr = XLMRobertaModel.from_pretrained(model_name)
        self.xlmr_hidden_size = self.xlmr.config.hidden_size
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.xlmr_hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        xlmr_output = self.xlmr(input_ids=input_ids, attention_mask=attention_mask)
        # XLM-RoBERTa doesn't have pooler_output, use last hidden state's [CLS] token
        cls_output = xlmr_output.last_hidden_state[:, 0, :]
        cls_output = self.dropout(cls_output)
        logits = self.fc(cls_output)
        return logits

# ============================================
# TRAINING AND EVALUATION FUNCTIONS
# ============================================

def train_epoch(model, data_loader, criterion, optimizer, device, scaler):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    progress_bar = tqdm(data_loader, desc="Training")

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        with torch.amp.autocast('cuda'):
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        _, preds = torch.max(logits, dim=1)
        correct_predictions += torch.sum(preds == labels).item()
        total_predictions += labels.size(0)
        running_loss += loss.item()

        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{correct_predictions / total_predictions:.4f}'
        })

    epoch_loss = running_loss / len(data_loader)
    epoch_acc = correct_predictions / total_predictions

    return epoch_loss, epoch_acc

def eval_model(model, data_loader, criterion, device):
    """Evaluate the model"""
    model.eval()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0
    all_preds = []
    all_labels = []

    progress_bar = tqdm(data_loader, desc="Validation")

    with torch.no_grad():
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            with torch.amp.autocast('cuda'):
                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)

            _, preds = torch.max(logits, dim=1)
            correct_predictions += torch.sum(preds == labels).item()
            total_predictions += labels.size(0)
            running_loss += loss.item()

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{correct_predictions / total_predictions:.4f}'
            })

    epoch_loss = running_loss / len(data_loader)
    epoch_acc = correct_predictions / total_predictions
    epoch_f1 = f1_score(all_labels, all_preds, average='weighted')

    return epoch_loss, epoch_acc, epoch_f1

def get_predictions(model, data_loader, device):
    """Get predictions for the entire dataset"""
    model.eval()
    all_predictions = []
    all_labels = []

    progress_bar = tqdm(data_loader, desc="Predicting")

    with torch.no_grad():
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            with torch.amp.autocast('cuda'):
                logits = model(input_ids, attention_mask)

            _, preds = torch.max(logits, dim=1)
            all_predictions.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_predictions), np.array(all_labels)

# ============================================
# VISUALIZATION FUNCTIONS
# ============================================

def plot_confusion_matrix(y_true, y_pred, class_labels):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_labels)))

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels,
                cbar_kws={'label': 'Count'})
    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')
    plt.ylabel('True Label', fontsize=12, fontweight='bold')
    plt.title('Confusion Matrix - XLM-RoBERTa', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    """Plot training history with F1 score"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Plot Loss
    axes[0].plot(history['train_loss'], label='Training Loss', marker='o', linewidth=2)
    axes[0].plot(history['val_loss'], label='Validation Loss', marker='s', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')
    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    axes[0].legend(loc='best', fontsize=10)
    axes[0].grid(True, alpha=0.3)

    # Plot Accuracy
    axes[1].plot(history['train_acc'], label='Training Accuracy', marker='o', linewidth=2)
    axes[1].plot(history['val_acc'], label='Validation Accuracy', marker='s', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')
    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    axes[1].legend(loc='best', fontsize=10)
    axes[1].grid(True, alpha=0.3)

    # Plot F1 Score
    axes[2].plot(history['val_f1'], label='Validation F1', marker='D', linewidth=2, color='green')
    axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')
    axes[2].set_ylabel('F1 Score', fontsize=12, fontweight='bold')
    axes[2].set_title('Validation F1 Score', fontsize=14, fontweight='bold')
    axes[2].legend(loc='best', fontsize=10)
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ============================================
# MAIN EXECUTION
# ============================================

def main():
    """Main execution function with optimized hyperparameters"""

    # Load and preprocess data
    print("\n" + "="*70)
    print("DATA LOADING AND PREPROCESSING")
    print("="*70)

    df = load_and_preprocess_data(Config.DATASET_PATH)
    plot_polarity_distribution(df)
    df = preprocess_dataframe(df)

    # Label encoding
    print("\nEncoding labels...")
    label_encoder = LabelEncoder()
    df['polarity'] = label_encoder.fit_transform(df['polarity'])
    class_labels = label_encoder.classes_

    print("\nEncoded Labels:")
    for label, encoded_value in zip(class_labels, label_encoder.transform(class_labels)):
        print(f"Polarity: {label} → Encoded: {encoded_value}")

    # Train-test split
    print(f"\n{'='*70}")
    print("DATASET SPLIT")
    print("="*70)

    X = df['text'].values
    y = df['polarity'].values

    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=Config.RANDOM_SEED
    )

    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=Config.RANDOM_SEED
    )

    print(f"Training samples: {len(X_train)}")
    print(f"Validation samples: {len(X_val)}")
    print(f"Test samples: {len(X_test)}")

    # Create datasets and dataloaders with optimized parameters
    print("\nLoading tokenizer and creating datasets...")
    tokenizer = XLMRobertaTokenizer.from_pretrained(Config.MODEL_NAME)

    train_dataset = SentimentDataset(X_train, y_train, tokenizer, Config.MAX_LEN)
    val_dataset = SentimentDataset(X_val, y_val, tokenizer, Config.MAX_LEN)
    test_dataset = SentimentDataset(X_test, y_test, tokenizer, Config.MAX_LEN)

    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)

    print(f"Training batches: {len(train_loader)}")
    print(f"Validation batches: {len(val_loader)}")
    print(f"Test batches: {len(test_loader)}")

    # Initialize model with optimized parameters
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*70}")
    print(f"DEVICE: {device}")
    print(f"{'='*70}")

    num_classes = len(class_labels)
    model = XLMRoberta_Classifier(
        Config.MODEL_NAME,
        num_classes,
        dropout=Config.DROPOUT
    ).to(device)

    print(f"\nModel: {Config.MODEL_NAME}")
    print(f"Number of classes: {num_classes}")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Training setup with optimized optimizer
    criterion = nn.CrossEntropyLoss()

    if Config.OPTIMIZER_TYPE == 'adamw':
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=Config.LEARNING_RATE,
            weight_decay=Config.WEIGHT_DECAY
        )
        print(f"\nOptimizer: AdamW")
    else:
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=Config.LEARNING_RATE,
            weight_decay=Config.WEIGHT_DECAY
        )
        print(f"\nOptimizer: Adam")

    print(f"Learning Rate: {Config.LEARNING_RATE:.2e}")
    print(f"Weight Decay: {Config.WEIGHT_DECAY:.6f}")

    scaler = torch.amp.GradScaler('cuda')
    early_stopping = EarlyStopping(patience=Config.PATIENCE, min_delta=Config.MIN_DELTA)

    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': []
    }

    # Training loop
    print("\n" + "="*70)
    print("STARTING TRAINING WITH OPTIMIZED HYPERPARAMETERS")
    print("="*70)
    print(f"Patience: {Config.PATIENCE} epochs")
    print(f"Min Delta: {Config.MIN_DELTA}")
    print("\n")

    best_val_f1 = 0.0

    for epoch in range(Config.EPOCHS):
        print(f"\nEpoch {epoch + 1}/{Config.EPOCHS}")
        print("-" * 70)

        # Train
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, scaler
        )

        # Validate
        val_loss, val_acc, val_f1 = eval_model(model, val_loader, criterion, device)

        # Store history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1'].append(val_f1)

        # Print epoch summary
        print(f"\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}")

        # Track best F1
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1

        # Early stopping check
        early_stopping(val_loss, epoch + 1)
        if early_stopping.early_stop:
            print(f"\n{'='*70}")
            print(f"Training stopped at epoch {epoch + 1}")
            print(f"Best model from epoch {early_stopping.best_epoch}")
            print(f"{'='*70}\n")
            break

    print("\n" + "="*70)
    print("TRAINING COMPLETED")
    print("="*70 + "\n")

    # Get predictions on test set
    print("Evaluating on test set...")
    y_pred, y_true = get_predictions(model, test_loader, device)

    test_accuracy = accuracy_score(y_true, y_pred)
    test_f1 = f1_score(y_true, y_pred, average='weighted')

    print(f"\n{'='*70}")
    print("FINAL TEST RESULTS WITH OPTIMIZED HYPERPARAMETERS")
    print(f"{'='*70}")
    print(f"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print(f"Test F1 Score: {test_f1:.4f}")
    print(f"{'='*70}\n")

    # Classification report
    print("\n" + "="*70)
    print("CLASSIFICATION REPORT")
    print("="*70 + "\n")

    target_names = [str(label) for label in class_labels]
    print(classification_report(y_true, y_pred, target_names=target_names))

    # Visualizations
    plot_confusion_matrix(y_true, y_pred, class_labels)
    plot_training_history(history)

    # Summary of optimized configuration
    print("\n" + "="*70)
    print("TRAINING SUMMARY - OPTIMIZED XLM-RoBERTa MODEL")
    print("="*70)
    print(f"Model: {Config.MODEL_NAME}")
    print(f"Optimized Hyperparameters:")
    print(f"  - Learning Rate:       {Config.LEARNING_RATE:.2e}")
    print(f"  - Batch Size:          {Config.BATCH_SIZE}")
    print(f"  - Max Length:          {Config.MAX_LEN}")
    print(f"  - Dropout:             {Config.DROPOUT:.4f}")
    print(f"  - Weight Decay:        {Config.WEIGHT_DECAY:.6f}")
    print(f"  - Optimizer:           {Config.OPTIMIZER_TYPE.upper()}")
    print(f"\nFinal Results:")
    print(f"  - Best Val F1:         {best_val_f1:.4f}")
    print(f"  - Test Accuracy:       {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print(f"  - Test F1 Score:       {test_f1:.4f}")
    print(f"  - Total Epochs:        {len(history['train_loss'])}")
    print("="*70)

    print("\n" + "="*70)
    print("ALL TASKS COMPLETED SUCCESSFULLY")
    print("="*70)

    return model, history, test_accuracy, test_f1

# Run main function
if __name__ == "__main__":
    model, history, test_acc, test_f1 = main()
